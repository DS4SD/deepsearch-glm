{
  "_name": "Applied AI Letters - 2020 - Staar.pdf",
  "_type": "pdf-document",
  "bitmaps": [],
  "description": {
    "logs": [
      {
        "agent": "CCS",
        "type": "parsing",
        "comment": "CCS v0.0.0-dev parsing of documents",
        "date": "2023-08-23T10:35:12.830027+00:00"
      }
    ]
  },
  "equations": [],
  "figures": [
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE1 Schematic of a data flow for the creation of a Knowledge Graph. The data flow consists of three main task types: extraction of document elements (abstracts, paragraphs, tables, figures, etc.), annotation of these elements to detect entities and their relationships and finally aggregation of these entities and their relationships. For every task, we keep complete provenance, such that we can always trace back to a specific document or element that embeds a certain entity or relationship",
      "prov": [
        {
          "bbox": [
            78.55,
            102.72,
            512.39,
            284.99
          ],
          "page": 3,
          "span": [
            0,
            498
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "",
      "prov": [
        {
          "bbox": [
            454.14,
            745.72,
            550.62,
            761.01
          ],
          "page": 5,
          "span": [
            0,
            0
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE 3 The time-to-solution for k-hop graph traversal for Neo4J and our new graph engine. The results were obtained for the graph500 and twitter benchmark graphs. The 10th and 90th percentiles are represented by the shaded regions; the median is shown by the markers",
      "prov": [
        {
          "bbox": [
            96.35,
            537.81,
            496.87,
            731.78
          ],
          "page": 8,
          "span": [
            0,
            268
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE 4 Visual workflow editor for deep queries in the CPS platform. The interface exhibits a left toolbar to pick specific graph operations, a main drawing area for the workflow DAG and a right panel to inspect and define parameters of each graph operation. Colors indicate different operation types such as input node-retrieval (blue), traversal (red), logical operators (green) and transform functions (yellow). Valid workflows can be executed using the ' play ' button",
      "prov": [
        {
          "bbox": [
            116.26,
            507.84,
            473.64,
            731.27
          ],
          "page": 9,
          "span": [
            0,
            473
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "",
      "prov": [
        {
          "bbox": [
            45.89,
            743.98,
            143.19,
            761.31
          ],
          "page": 10,
          "span": [
            0,
            0
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE5 The architectural design of the CPS platform. On the left, we show the data flow processing architecture orchestrated through an asynchronous REST API. On the right, we sketch the multitenant KG serving facility which provides a dedicated environment for each project",
      "prov": [
        {
          "bbox": [
            48.37,
            477.84,
            548.36,
            732.33
          ],
          "page": 11,
          "span": [
            0,
            275
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE 6 Sketch of the entire pipeline to perform deep data exploration on large corpora",
      "prov": [
        {
          "bbox": [
            55.88,
            606.85,
            541.85,
            729.68
          ],
          "page": 12,
          "span": [
            0,
            88
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "FIGURE 7 The evaluation workflow to identify the petroleum system elements (PSE) in an article and infer its properties. It starts by searching for all petroleum system elements of a certain type (eg, source, reservoir or seal) and a particular report (worktasks 1 and 2). By successive graph traversals (worktasks 3-5, 7-9, 11, 12) along specific edges and logical operations (worktasks 6, 10, 13, 14), we are able to obtain a list of candidate formations (worktask 15), ages (worktask 16) and rocks (worktask 17), ranked by their accumulated weight. Execution of this query takes less than 18 ms on average",
      "prov": [
        {
          "bbox": [
            45.16,
            607.38,
            548.95,
            731.49
          ],
          "page": 13,
          "span": [
            0,
            608
          ]
        }
      ]
    },
    {
      "confidence": 0.0,
      "created_by": "unknown",
      "type": "figure",
      "cells": [],
      "data": [],
      "text": "",
      "prov": [
        {
          "bbox": [
            454.56,
            745.46,
            549.1,
            761.86
          ],
          "page": 15,
          "span": [
            0,
            0
          ]
        }
      ]
    }
  ],
  "file-info": {
    "#-pages": 15,
    "document-hash": "457bcbb2d189b4719daa30d94d946d913f1a6bddaabd1c12793b143a30e1115d",
    "filename": "Applied AI Letters - 2020 - Staar.pdf",
    "page-hashes": [
      {
        "hash": "365f2c5695b6aa22d2096bc88ab18f76b2c7f3eed6359d68af14a6aa7916ce70",
        "model": "model",
        "page": 1
      },
      {
        "hash": "4fe5aedd95efb3beca7a5b39c9d9883838b8f5ac746d167c3fbfb4e9848d4a7e",
        "model": "model",
        "page": 2
      },
      {
        "hash": "f3d1a1b3bad4dce4ab96bdf3e3fe105a7814009d24bacbfe5b90737979503e01",
        "model": "model",
        "page": 3
      },
      {
        "hash": "dd8ebce06c6e2bec4a188b005eab757b6a222b7ae632eeec943767dbac6573b8",
        "model": "model",
        "page": 4
      },
      {
        "hash": "0ebc61b0f17fd0a49cb6fd809500169f2fb13a27f15dfd345e8099c7e45491ed",
        "model": "model",
        "page": 5
      },
      {
        "hash": "39a00bc2dd440023ebc606e26683a79bf9f6806f61af143cb1edfd03eba28a14",
        "model": "model",
        "page": 6
      },
      {
        "hash": "5cfb4312275e9ed166e878f8b557709d831899591c7414facbf8d0fdd732d4ce",
        "model": "model",
        "page": 7
      },
      {
        "hash": "71916575c81d0609e6862ac97c00dad85d1ec468262ea3934db42b8f13f0e7e7",
        "model": "model",
        "page": 8
      },
      {
        "hash": "df3fce65445de0d07ebfda8634f70566417a0fcdd249c0120056aa082f1b1733",
        "model": "model",
        "page": 9
      },
      {
        "hash": "03a8792b4ca1b1e832dec33cd23345c446d214f94b29e174946e062d890555e0",
        "model": "model",
        "page": 10
      },
      {
        "hash": "b69f19266a3247e8e43ce2fdefba2018e63d827c38c8758ee70c51426a78c651",
        "model": "model",
        "page": 11
      },
      {
        "hash": "53eb4a9c44cc911454a0a1bc45ffc8261737cdb47f43e2d97cc477ff1912c2a5",
        "model": "model",
        "page": 12
      },
      {
        "hash": "56679ab44cd3f3e11de5886ccf872fb064cee7916809fa78e02a87cfab71a8e1",
        "model": "model",
        "page": 13
      },
      {
        "hash": "6888730ff9f5a52a88ec198c50365f020fc78bca19bbdd1482bd5059172da68c",
        "model": "model",
        "page": 14
      },
      {
        "hash": "0ff9d40eaca3f08f5b36c3644bf466c37a0644bd71a71066425200792dcff82d",
        "model": "model",
        "page": 15
      }
    ]
  },
  "footnotes": [],
  "main-text": [
    {
      "text": "Received: 15 September 2020",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.79,
            743.58,
            131.78,
            750.79
          ],
          "page": 1,
          "span": [
            0,
            27
          ]
        }
      ]
    },
    {
      "text": "Revised: 23 November 2020",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            146.33,
            744.09,
            229.31,
            751.44
          ],
          "page": 1,
          "span": [
            0,
            25
          ]
        }
      ]
    },
    {
      "text": "Accepted: 25 November 2020",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            243.78,
            743.95,
            332.99,
            751.35
          ],
          "page": 1,
          "span": [
            0,
            26
          ]
        }
      ]
    },
    {
      "text": "DOI: 10.1002/ail2.20",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.69,
            730.71,
            106.12,
            737.3
          ],
          "page": 1,
          "span": [
            0,
            20
          ]
        }
      ]
    },
    {
      "text": "LETTER",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            43.96,
            702.4,
            91.95,
            712.1
          ],
          "page": 1,
          "span": [
            0,
            6
          ]
        }
      ]
    },
    {
      "text": "Corpus processing service: A Knowledge Graph platform to perform deep data exploration on corpora",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.71,
            631.27,
            520.77,
            672.01
          ],
          "page": 1,
          "span": [
            0,
            97
          ]
        }
      ]
    },
    {
      "text": "Peter W. J. Staar",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.79,
            593.61,
            146.47,
            606.47
          ],
          "page": 1,
          "span": [
            0,
            17
          ]
        }
      ]
    },
    {
      "text": "|",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            160.1,
            593.72,
            163.59,
            605.11
          ],
          "page": 1,
          "span": [
            0,
            1
          ]
        }
      ]
    },
    {
      "text": "Michele Dolfi",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            170.39,
            593.44,
            265.12,
            607.21
          ],
          "page": 1,
          "span": [
            0,
            13
          ]
        }
      ]
    },
    {
      "text": "|",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            274.56,
            593.72,
            278.06,
            605.11
          ],
          "page": 1,
          "span": [
            0,
            1
          ]
        }
      ]
    },
    {
      "text": "Christoph Auer",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            290.04,
            593.26,
            387.63,
            606.96
          ],
          "page": 1,
          "span": [
            0,
            14
          ]
        }
      ]
    },
    {
      "text": "IBM Research, Rueschlikon, Switzerland",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            559.6,
            182.68,
            567.3
          ],
          "page": 1,
          "span": [
            0,
            38
          ]
        }
      ]
    },
    {
      "text": "Correspondence Peter W. J. Staar, IBM Research, Saumerstrasse 4, 8820 Rueschlikon, Switzerland. Email: taa@zurich.ibm.com",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            493.49,
            164.66,
            545.31
          ],
          "page": 1,
          "span": [
            0,
            121
          ]
        }
      ]
    },
    {
      "text": "Abstract",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            209.19,
            552.25,
            249.13,
            561.74
          ],
          "page": 1,
          "span": [
            0,
            8
          ]
        }
      ]
    },
    {
      "text": "Knowledge Graphs have been fast emerging as the de facto standard to model and explore knowledge in weakly structured data. Large corpora of documents constitute a source of weakly structured data of particular interest for both the academic and business world. Key examples include scientific publications, technical reports, manuals, patents, regulations, etc. Such corpora embed many facts that are elementary to critical decision making or enabling new discoveries. In this paper, we present a scalable cloud platform to create and serve Knowledge Graphs, which we named corpus processing service (CPS). Its purpose is to process large document corpora, extract the content and embedded facts, and ultimately represent these in a consistent knowledge graph that can be intuitively queried. To accomplish this, we use state-of-the-art natural language understanding models to extract entities and relationships from documents converted with our previously presented corpus conversion service platform. This pipeline is complemented with a newly developed graph engine which ensures extremely performant graph queries and provides powerful graph analytics capabilities. Both components are tightly integrated and can be easily consumed through REST APIs. Additionally, we provide user interfaces to control the data ingestion flow and formulate queries using a visual programming approach. The CPS platform is designed as a modular microservice system operating on Kubernetes clusters. Finally, we validate the quality of queries on our endto-end knowledge pipeline in a real-world application in the oil and gas industry.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            208.61,
            251.59,
            543.86,
            547.04
          ],
          "page": 1,
          "span": [
            0,
            1624
          ]
        }
      ]
    },
    {
      "text": "KEYWORDS",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            209.21,
            228.2,
            269.01,
            237.28
          ],
          "page": 1,
          "span": [
            0,
            8
          ]
        }
      ]
    },
    {
      "text": "document processing, knowledge graph, semantic search",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            208.8,
            214.08,
            401.03,
            222.97
          ],
          "page": 1,
          "span": [
            0,
            53
          ]
        }
      ]
    },
    {
      "text": "1 | INTRODUCTION",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.28,
            187.52,
            189.72,
            199.66
          ],
          "page": 1,
          "span": [
            0,
            16
          ]
        }
      ]
    },
    {
      "text": "As of 2015, Adobe estimated that there were 2.7 trillion PDF documents in circulation globally. It is self-evident that this number has increased ever since. The explosive growth of documents one can observe since digital publishing became mainstream is posing a serious challenge to both the academic and corporate world. The increased publication rate of scientific articles makes it harder and harder for academics to keep aware of all the latest findings. Similarly, the ever-growing number of internal reports, documentation, patents, contracts, regulations, court filings, etc., is for most corporations becoming simply unmanageable.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            96.98,
            552.65,
            172.33
          ],
          "page": 1,
          "span": [
            0,
            639
          ]
        }
      ]
    },
    {
      "text": "This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.",
      "name": "footnote",
      "type": "footnote",
      "prov": [
        {
          "bbox": [
            44.79,
            52.5,
            540.7,
            70.33
          ],
          "page": 1,
          "span": [
            0,
            201
          ]
        }
      ]
    },
    {
      "text": "\u00a9 2020 The Authors. Applied AI Letters published by John Wiley & Sons Ltd.",
      "name": "footnote",
      "type": "footnote",
      "prov": [
        {
          "bbox": [
            44.79,
            42.45,
            272.17,
            50.21
          ],
          "page": 1,
          "span": [
            0,
            74
          ]
        }
      ]
    },
    {
      "text": "Applied AI Letters. 2020;1:e20. https://doi.org/10.1002/ail2.20",
      "name": "page-footer",
      "type": "page-footer",
      "prov": [
        {
          "bbox": [
            44.38,
            12.3,
            135.59,
            30.87
          ],
          "page": 1,
          "span": [
            0,
            63
          ]
        }
      ]
    },
    {
      "text": "wileyonlinelibrary.com/journal/ail2 1of15",
      "name": "page-footer",
      "type": "page-footer",
      "prov": [
        {
          "bbox": [
            400.53,
            22.28,
            550.62,
            29.7
          ],
          "page": 1,
          "span": [
            0,
            41
          ]
        }
      ]
    },
    {
      "text": "2of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            751.41,
            68.56,
            758.05
          ],
          "page": 2,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.96,
            758.33
          ],
          "page": 2,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "In a previous publication, we presented the corpus conversion service (CCS). 1 The CCS is a scalable cloud service, which leverages state-of-the-art machine learning to convert complex formats (eg, PDF, Word, and Bitmap) into a richly structured JSON representation of their content. As such, the CCS solves the first problem when confronted with a large corpus of documents, that is, make the content of the documents programmatically accessible. Examples of the latter would be ' List all images with their caption from the corpus or list all titles with their publication date. ' The second problem is to obviously search or explore the content of the documents in a large corpus. For this problem, we have developed the corpus processing service (CPS), which we present in this paper. The CPS is intended to create knowledge bases (KBs) from the converted JSON corpus and serve these KBs through in-memory knowledge graph stores. As such, the CPS is the natural extension of the CCS and has as an express purpose to make corpora of documents available for deep data exploration.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.97,
            604.04,
            554.34,
            732.59
          ],
          "page": 2,
          "span": [
            0,
            1082
          ]
        }
      ]
    },
    {
      "text": "The purpose of CPS is to enable deep data exploration directly on large corpora. Here, we define deep data exploration as the capability to ingest large corpora of documents into a scalable service and detect, extract and combine facts contained in these corpora in order to make new discoveries or support critical decision making. It is key to understand that our goal of creating and querying Knowledge Graphs to enable deep data exploration goes beyond search in the spirit of rank and retrieve. Although search is by no means trivial, many state-of-the art solutions exist for this purpose. * We argue, however, that one needs query capabilities which allow for a combination of extracted facts and a fast, onthe-fly creation of new datasets to enable actual deep data exploration. Those datasets can then be used for further anal-",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            513.05,
            553.24,
            601.04
          ],
          "page": 2,
          "span": [
            0,
            836
          ]
        }
      ]
    },
    {
      "text": "ysis, which might lead to new discoveries or support decision making.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            500.06,
            340.6,
            509.47
          ],
          "page": 2,
          "span": [
            0,
            69
          ]
        }
      ]
    },
    {
      "text": "To better distinguish this approach from conventional search, let us consider some example questions:",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            57.86,
            487.08,
            492.16,
            496.64
          ],
          "page": 2,
          "span": [
            0,
            101
          ]
        }
      ]
    },
    {
      "text": "a. Definition of high temperature superconductor.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            461.06,
            262.57,
            470.57
          ],
          "page": 2,
          "span": [
            0,
            49
          ]
        }
      ]
    },
    {
      "text": "b. Publications of <person> before year 2010.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.78,
            448.07,
            241.75,
            457.51
          ],
          "page": 2,
          "span": [
            0,
            45
          ]
        }
      ]
    },
    {
      "text": "c. Maps of the Permian basin.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            435.03,
            174.96,
            444.55
          ],
          "page": 2,
          "span": [
            0,
            29
          ]
        }
      ]
    },
    {
      "text": "d. Geological formations from the Miocene age with their depth, thickness, geographic location, and composition.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            422.05,
            528.81,
            431.55
          ],
          "page": 2,
          "span": [
            0,
            112
          ]
        }
      ]
    },
    {
      "text": "e. List all high-Tc superconductors with their known crystallographic and material properties?",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.39,
            409.07,
            446.48,
            418.9
          ],
          "page": 2,
          "span": [
            0,
            94
          ]
        }
      ]
    },
    {
      "text": "Question (a) undoubtedly fits the classic search paradigm, since here one can expect a search engine to find a number sources with exact answers (ie, definitions). Likewise, question (b) can be easily answered through metadata based filter rules on a literature database. Question (c) already requires some extent of domain knowledge to be encoded in a model to accurately classify the relevance of all known maps to the query, at least assuming no manual curation effort has been done. Questions (d) and (e) ultimately impose query capabilities which are clearly infeasible to support through manual curation, and are very unlikely to be answered in any single data source. These questions require the system to return a more complex data structure (eg, a table in which the rows list the formations or materials while the columns contain their respective properties).",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.0,
            292.05,
            553.06,
            392.7
          ],
          "page": 2,
          "span": [
            0,
            869
          ]
        }
      ]
    },
    {
      "text": "Concluding from the above examples, we define the following qualifying criteria for a system that supports deep data exploration on corpora:",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            265.89,
            551.48,
            288.82
          ],
          "page": 2,
          "span": [
            0,
            140
          ]
        }
      ]
    },
    {
      "text": "1. It can answer queries by combining different data elements from different sources into a new data structure.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.37,
            240.06,
            515.49,
            249.53
          ],
          "page": 2,
          "span": [
            0,
            111
          ]
        }
      ]
    },
    {
      "text": "2. It supports (1) by creating a knowledge model from a controlled, unstructured corpus in a mostly unsupervised way. It may profit from, but not require any manually curated data.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            214.04,
            551.05,
            236.59
          ],
          "page": 2,
          "span": [
            0,
            180
          ]
        }
      ]
    },
    {
      "text": "3. It may restrict supported queries to a specific domain (eg, a technical field).",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.2,
            201.06,
            376.77,
            210.76
          ],
          "page": 2,
          "span": [
            0,
            82
          ]
        }
      ]
    },
    {
      "text": "To meet the objectives defined earlier, CPS implements and tightly integrates two essential components. The first component is a scalable Knowledge Graph creation pipeline, which is used to automatically process text, tables and images through state-of-the-art segmentation and natural language understanding (NLU) models and extract entities and relationships from them. The second component serves the created KG, enabling users to perform deep queries and advanced graph analytics in real time. 2 This is supported through an underlying, highly optimized graph engine we developed to specifically address requirements for deep data exploration.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.24,
            110.07,
            553.14,
            184.78
          ],
          "page": 2,
          "span": [
            0,
            647
          ]
        }
      ]
    },
    {
      "text": "It is worth noting that the CPS platform is a fully functioning cloud application that has been successfully deployed in multiple real-world scenarios in material science 3 and oil and gas industries. 4",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            84.05,
            550.51,
            107.71
          ],
          "page": 2,
          "span": [
            0,
            202
          ]
        }
      ]
    },
    {
      "text": "In the remainder of this paper, we discuss in detail the technical aspects and implementation details of the two main components of the CPS. In section 2, we present in depth how the platform extracts facts from corpora at a massive scale. In section 3, we go into detail of designing deep queries and show how we compute them in a very efficient",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.98,
            45.05,
            551.84,
            81.25
          ],
          "page": 2,
          "span": [
            0,
            346
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 2,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.51,
            751.46,
            85.02,
            758.05
          ],
          "page": 3,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "3of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            528.55,
            751.41,
            550.62,
            758.05
          ],
          "page": 3,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "way with our high-performance graph engine. Later, in section 4, we will discuss in detail how both components are deployed and interacting on the cloud. Finally, in section 5, we present the complete system in a real world case study and benchmark its accuracy.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            695.05,
            549.41,
            730.46
          ],
          "page": 3,
          "span": [
            0,
            262
          ]
        }
      ]
    },
    {
      "text": "2 | SCALABLE KNOWLEDGE GRAPH CREATION",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.79,
            655.52,
            378.15,
            666.9
          ],
          "page": 3,
          "span": [
            0,
            37
          ]
        }
      ]
    },
    {
      "text": "In CPS, a Knowledge Graph is defined as a collection of entities and their relationships forming the graphs nodes and edges. Entities can have a wide variety of types. A basic scenario includes types such as documents, document components, keywords, and authors. In addition, there can be more specific types tied to domain verticals, such as materials and properties in material science, or geological ages, formations, rocks, minerals, structures, etc., for oil and gas exploration. Relationships in the KG are strictly defined between the entities. Similar to the entities, the relationships are typed (' has-material-property ' or ' has-geological-age '). Also, relationships in the KG can be weighted, for example, to represent the trustworthiness of a fact that the relationship represents.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            552.05,
            549.78,
            639.58
          ],
          "page": 3,
          "span": [
            0,
            796
          ]
        }
      ]
    },
    {
      "text": "In typical cases, we start from a collection of documents in different formats. Sometimes, documents are available in semistructured, machine-interpretable formatssuchasJSON,XML,orHTML.However,inthevastmajority of cases this does not apply, especially for proprietary documents of companies and organizations. The latter are very often scanned or programmatic PDF documents. Using the CCS, 1 these types of documents are converted into structured JSON files. Those provide easy access to the meta-data (eg, title, abstract, references, authors) and the document body. The latter is structured by subtitles (of various levels), paragraphs, lists, tables (with internal row and column structures), figures, and linked captions. O n c et h ec o r p u si sp r e s n ti nas t r u c t u r e d,m a c h i n e processableformat,theKGiscreatedbyapplyingthreedistincttasks,namely extraction, annotation,and aggregation. The inherent dependencies between these three tasks are defined through a directed acyclic graph (DAG). We willrefertothisDAGoftasksasadataflow(DF).Inthenextsections,weestablishtheconceptofDFsanddiscuss the details for each DF task.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            409.07,
            554.41,
            548.48
          ],
          "page": 3,
          "span": [
            0,
            1141
          ]
        }
      ]
    },
    {
      "text": "2.1 | DF tasks",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.79,
            369.5,
            134.89,
            380.89
          ],
          "page": 3,
          "span": [
            0,
            14
          ]
        }
      ]
    },
    {
      "text": "In Figure 1, we sketch a minimal DF, in which each of the three tasks is used consecutively in order to generate entities and relationships for a generic KG. We will use Figure1toillustratethepurposeandimplementationof each DF task.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.52,
            317.65,
            552.39,
            353.52
          ],
          "page": 3,
          "span": [
            0,
            232
          ]
        }
      ]
    },
    {
      "text": "FIGURE1 Schematic of a data flow for the creation of a Knowledge Graph. The data flow consists of three main task types: extraction of document elements (abstracts, paragraphs, tables, figures, etc.), annotation of these elements to detect entities and their relationships and finally aggregation of these entities and their relationships. For every task, we keep complete provenance, such that we can always trace back to a specific document or element that embeds a certain entity or relationship",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            44.78,
            45.4,
            545.79,
            89.47
          ],
          "page": 3,
          "span": [
            0,
            498
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/0",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 3,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "4of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            751.41,
            68.56,
            758.05
          ],
          "page": 4,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.94,
            758.49
          ],
          "page": 4,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "2.1.1 | Extraction",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.14,
            720.49,
            157.76,
            732.34
          ],
          "page": 4,
          "span": [
            0,
            18
          ]
        }
      ]
    },
    {
      "text": "In an extraction task, we generate new data entities (eg, document components) from an original set of source entities (eg, documents). During this process, new links are created which connect these newly generated data entities to their original source entity. Typical examples of such extraction tasks are the extraction of abstracts, paragraphs, tables, or figures from the structured document files.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            656.08,
            553.55,
            704.77
          ],
          "page": 4,
          "span": [
            0,
            403
          ]
        }
      ]
    },
    {
      "text": "From a scalability point of view, this task is embarrassingly parallel, which makes it extremely easy to implement on loosely interconnected environments such as a cloud. We simply iterate in parallel over all source entities in the backend database, extract the desired components and then insert those components as new data entities back into the database. Extraction tasks have no internal synchronization points.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.56,
            604.04,
            553.09,
            652.89
          ],
          "page": 4,
          "span": [
            0,
            417
          ]
        }
      ]
    },
    {
      "text": "One particular benefit of this task is to make the query capability on the Knowledge Graph more fine grained by being able to provide provenance information on the result. For example, this would let the user explore all the paragraphs, tables, or figures that embed a certain fact.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.66,
            565.09,
            552.86,
            600.94
          ],
          "page": 4,
          "span": [
            0,
            282
          ]
        }
      ]
    },
    {
      "text": "2.1.2 | Annotation",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.5,
            525.52,
            161.91,
            536.91
          ],
          "page": 4,
          "span": [
            0,
            18
          ]
        }
      ]
    },
    {
      "text": "In the annotation task, we apply NLU methods to detect language entities and their relationships within a single data entity. Here, data entities can be as simple as a snippet of text (eg, a paragraph) or more complex structures such as tables or figures. The main goal of the annotation task is to obtain all relevant information from the data entity with regard to the domain of the corpus. Since different technical fields require different annotations, our annotation task is modular, allowing language entities to be annotated for material science, oil and gas, or more basic entities (eg, noun phrases, abbreviations, unit and values, etc.).",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.28,
            435.03,
            552.78,
            509.81
          ],
          "page": 4,
          "span": [
            0,
            647
          ]
        }
      ]
    },
    {
      "text": "From a technical perspective, the language entities are detected and annotated using multiple NLU methods, ranging from complex regular expressions \u2020 to LSTM networks. 5,6 We employ state-of-the-art NLU toolkits such as Spacy 7 or NLTK \u2021 to train and apply custom named entity recognition models. A detailed investigation of these NLU annotators unfortunately goes beyond of the scope of this paper. However, in Figure 2, we show the different types of named (geological) entities found in a paragraph by our oil and gas annotation model.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.0,
            370.07,
            551.75,
            431.6
          ],
          "page": 4,
          "span": [
            0,
            538
          ]
        }
      ]
    },
    {
      "text": "In Listing 1, we also show an excerpt of how the annotations (both language entities and relationships) are stored in the backend. It is noteworthy here that relationships are stored as (weighted) links between two entity references. \u00a7 The usage of references reduces data duplication and more importantly ensures that the relationships are always defined between two known entities in the KG. The latter simplifies the aggregation of the relationships significantly, since no new entities need to be created in the KG in order to aggregate the relationships (see section 2.1.4).",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.38,
            304.92,
            551.43,
            366.63
          ],
          "page": 4,
          "span": [
            0,
            579
          ]
        }
      ]
    },
    {
      "text": "FIGURE 2 Illustration of various detected language entities in a particularly rich snippet of an AAPG abstract. 8 The language entities here are all related to geological concepts in the domain of oil and gas exploration",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            45.4,
            540.32,
            67.21
          ],
          "page": 4,
          "span": [
            0,
            220
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 4,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.04,
            751.31,
            85.72,
            759.73
          ],
          "page": 5,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/1",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "LISTING 1 Excerpt of the annotated abstract from an AAPG paper 8 with its original text and the detected entities and relationships. Note that relationships are typed (encoded in the field name) and weighted. The weight reflects the confidence of the language annotation model during extraction. Relationships are always defined on detected entities, and will therefore use references defining a link between two entities",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            483.4,
            548.26,
            529.32
          ],
          "page": 5,
          "span": [
            0,
            421
          ]
        }
      ]
    },
    {
      "text": "From a scaling perspective, this task is again embarrassingly parallel. Unlike the extraction task, the annotation task is not creating new data entities, but rather appending new data associated with an existing data entity. We simply apply the desired entity and relationship annotators on all document components (paragraphs, tables, etc.) in parallel by distributing the operations on all available compute resources. Annotation tasks have no internal synchronization points. From a corpus of about 100 000 documents, we typically extract about 3 million paragraphs. Assuming unlimited resources, the annotation task could be distributed to potentially 3 million independent workers.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            370.06,
            549.87,
            444.57
          ],
          "page": 5,
          "span": [
            0,
            687
          ]
        }
      ]
    },
    {
      "text": "2.1.3 | Aggregation of entities",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.21,
            330.49,
            223.93,
            341.88
          ],
          "page": 5,
          "span": [
            0,
            31
          ]
        }
      ]
    },
    {
      "text": "The aggregation task for entities is similar to an extraction task, in the sense that we create new entities and link them each to the source they were mentioned in. In addition to extraction, the entity aggregation task also applies a similarity metric \u00b6 between the entities during extraction. This similarity metric will define if two entities refer to the same language concept and thus need to be represented by a single entity in the KG, rather than remaining separated. In Figure 1, we have illustrated the aggregation task for two types of entities across many different document components. These entity types could be for example materials and properties or geological formations and geological ages. The links connecting the new entities to their source entity are weighted according to the frequency of the match, that is, we set a higher weight if the language entity has been found multiple times. From an implementation point of view, the aggregation task for entities is nontrivial. In distributed computing, it corresponds to a reduction operation. Our implementation distributes the iteration of the source elements among all available computational resources. The aggregation is first performed in a local buffer, which is then synchronized with the backend database only when it reaches a maximum size. The synchronization step is a simple atomic update into an existing (or a newly created) database object. The synchronization for updates from each worker task does not collide with the others.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            149.07,
            549.82,
            314.54
          ],
          "page": 5,
          "span": [
            0,
            1516
          ]
        }
      ]
    },
    {
      "text": "2.1.4 | Aggregation of relationships",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            43.95,
            109.51,
            254.48,
            120.89
          ],
          "page": 5,
          "span": [
            0,
            36
          ]
        }
      ]
    },
    {
      "text": "The aggregation of relationships introduces new links between the entities that were aggregated in the previous aggregation operation. In Figure 1, this task is depicted as the last operation, where entities with an annotated relationship are explicitly linked together. For example, we create an edge between the Egret-Hibernia Petroleum System and Jeanne D'Arc Basin from Listing 1.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            45.01,
            549.14,
            93.61
          ],
          "page": 5,
          "span": [
            0,
            384
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 5,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "6of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            751.41,
            68.56,
            758.05
          ],
          "page": 6,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.99,
            758.98
          ],
          "page": 6,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "Similar to the aggregation of entities, the aggregation task for relationships is a reduction operation. Two independent document components could describe the same relationship between two entities. To minimize the synchronization lookup operation with the backend database, this task also utilizes a local buffer which accumulates the changes to be committed to the KG until the maximum size is reached. This approach allows to distribute the computation among all the source document components and performs very few blocking operations in the backend database.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.78,
            669.06,
            554.4,
            730.82
          ],
          "page": 6,
          "span": [
            0,
            564
          ]
        }
      ]
    },
    {
      "text": "2.2 | Data flows",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.75,
            629.49,
            148.0,
            641.57
          ],
          "page": 6,
          "span": [
            0,
            16
          ]
        }
      ]
    },
    {
      "text": "The purpose of a DF is to provide an execution plan for the task types detailed above in a meaningful order to generate or update a specific KG. When instantiating a DF, one has the possibility to define in a declarative way:",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            591.05,
            552.9,
            613.81
          ],
          "page": 6,
          "span": [
            0,
            225
          ]
        }
      ]
    },
    {
      "text": "1. Which document components should be extracted from a converted corpus to form source entities (eg, extract all paragraphs, tables, figures and captions from the AAPG articles)?",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.45,
            552.05,
            553.36,
            575.29
          ],
          "page": 6,
          "span": [
            0,
            179
          ]
        }
      ]
    },
    {
      "text": "2. Which annotator model(s) to use on which type of source entity (eg, run the geology or material science annotators on paragraphs)?",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.74,
            526.08,
            553.54,
            548.9
          ],
          "page": 6,
          "span": [
            0,
            133
          ]
        }
      ]
    },
    {
      "text": "3. Which entity and relationship aggregations to perform on which set of annotated language entities?",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.88,
            513.04,
            481.36,
            523.51
          ],
          "page": 6,
          "span": [
            0,
            101
          ]
        }
      ]
    },
    {
      "text": "The DFs can thus be seen as blueprints for processing the corpus into a defined graph topology. Notably, our implementation of DFs and their tasks retains the flexibility of processing not only source documents of a well-known data schema such as from CCS, but virtually any structure that can be transformed to a JSON representation, including data entities from precurated databases. We designed the CPS platform to support export and import of DFs on entirely new datasets without the burden of recreating it from scratch.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.39,
            435.03,
            553.39,
            497.02
          ],
          "page": 6,
          "span": [
            0,
            525
          ]
        }
      ]
    },
    {
      "text": "Our backend engine can exploit the DAG defined through the DF to massively distribute the individual tasks on all compute resources, because independent branches of the DAG each containing a chain of tasks can execute in parallel. The achievable level of parallelism changes throughout the execution. A practical example is a DF which extracts paragraphs and abstracts from all documents in the corpus, then annotates them and finally aggregates all entities. Here, the extraction tasks are distributed only over all documents; then, in the annotation tasks, we increase the parallelism to all document components. Any synchronization points thus can be pushed back into the aggregation tasks.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.55,
            344.04,
            555.01,
            432.12
          ],
          "page": 6,
          "span": [
            0,
            693
          ]
        }
      ]
    },
    {
      "text": "3 | DEEP DATA EXPLORATION USING KNOWLEDGE GRAPHS",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            46.26,
            304.47,
            469.55,
            315.86
          ],
          "page": 6,
          "span": [
            0,
            48
          ]
        }
      ]
    },
    {
      "text": "We will now look into the requirements to perform deep data exploration on a populated Knowledge Graph. A deep data exploration requires two fundamental capabilities:",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            265.93,
            552.64,
            288.61
          ],
          "page": 6,
          "span": [
            0,
            166
          ]
        }
      ]
    },
    {
      "text": "1. perform deep queries on the graph, that is, queries that require multi-hop traversals and",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.38,
            240.05,
            429.52,
            249.76
          ],
          "page": 6,
          "span": [
            0,
            92
          ]
        }
      ]
    },
    {
      "text": "2. perform graph analytics on the full graph or subsets of it on-the-fly.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.62,
            227.09,
            346.36,
            237.67
          ],
          "page": 6,
          "span": [
            0,
            73
          ]
        }
      ]
    },
    {
      "text": "Deep queries are essential to dynamically combine independent facts together in the given query context. This would apply for example to explorational queries aimed to characterize petroleum system elements, as detailed in our case study (see section 5). Graph analytics can further reveal hidden structure in the KG topology. Examples of advanced graphanalytical operations are page rank, node centralities, 9,10 node clustering, spectral analysis, and label propagation.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.32,
            162.06,
            553.89,
            210.65
          ],
          "page": 6,
          "span": [
            0,
            472
          ]
        }
      ]
    },
    {
      "text": "Both deep queries and graph analytics have in common that they are inherently expensive to compute on conventional graph databases, due to a rapid expansion of the number of visited nodes as a function of the graph-traversal depth. This is a major obstacle in providing reasonable time-to-solution in the aforementioned cases. Virtually all established graph database products on the market today ** fall victim to this, as was also reported in multiple sources. 11,12 Due to the poor performance we observed with available graph databases, we developed a new graph engine for the CPS platform. This graph engine is able to execute advanced graph-analytics 2 as well as evaluate deep queries with multi-hop traversals on large graphs (>1B edges) extremely fast.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.76,
            71.07,
            554.23,
            158.8
          ],
          "page": 6,
          "span": [
            0,
            761
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 6,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.35,
            751.46,
            85.42,
            758.93
          ],
          "page": 7,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "7of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            528.55,
            751.41,
            550.62,
            758.05
          ],
          "page": 7,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "In the remaining part of this section, we elaborate on our newly developed graph engine. In section 3.1, we discuss the implementation design. In section 3.2, we discuss performance results and compare it to Neo4J. Later, in section 3.3, we will explain how the deep queries are formulated and evaluated in the graph engine.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            695.09,
            549.55,
            730.67
          ],
          "page": 7,
          "span": [
            0,
            324
          ]
        }
      ]
    },
    {
      "text": "3.1 | Design of the graph engine",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.72,
            655.52,
            236.79,
            666.9
          ],
          "page": 7,
          "span": [
            0,
            32
          ]
        }
      ]
    },
    {
      "text": "In computer science, two prevalent implementation schemes for graphs have emerged, one using adjacency lists and one relying on adjacency matrices. 13,14 In the adjacency list format, every node is essentially an object which contains a set of indices representing its neighbors. \u2020\u2020 The edges are therefore stored as a property of the node. In the adjacency matrix approach, all nodes obtain an identifier (typically an unsigned integer) and the edges are stored as a list of nodeidentifier tuples.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            578.07,
            549.25,
            640.17
          ],
          "page": 7,
          "span": [
            0,
            498
          ]
        }
      ]
    },
    {
      "text": "It is commonly known that most graph operations can be translated into matrix-operations using linear algebra. 13 For example, consider the graph-traversal V ! A W, in which we start from a set of nodes V and traverse the edge A in order to obtain a new set of nodes W. This can be directly translated into linear algebra as",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.73,
            539.07,
            548.86,
            576.57
          ],
          "page": 7,
          "span": [
            0,
            324
          ]
        }
      ]
    },
    {
      "text": "w $^{!}$= Av ! with v $^{!}$$_{i}$= 1 if node i \\b V 0 if node i = 2 V , GLYPH<C26> \u00f0 1 \u00de",
      "name": "formula",
      "type": "equation",
      "prov": [
        {
          "bbox": [
            214.75,
            498.59,
            548.78,
            529.37
          ],
          "page": 7,
          "span": [
            0,
            89
          ]
        }
      ]
    },
    {
      "text": "and with A being the adjacency matrix representation of the edge A. Translating single graph-traversals into linear algebra operations significantly simplifies the job of deeper graph traversals. For example, to obtain the k-order neighborhood of node set V, one simply needs to evaluate Equation (1) k times recursively, as in",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.78,
            435.04,
            548.75,
            470.53
          ],
          "page": 7,
          "span": [
            0,
            327
          ]
        }
      ]
    },
    {
      "text": "w $^{!}$= A$^{k}$v $^{!}$= AA \u2026 Av ! GLYPH<C16>GLYPH<C17> GLYPH<C16> GLYPH<C17> GLYPH<C16> GLYPH<C17> : \u00f0 2 \u00de",
      "name": "formula",
      "type": "equation",
      "prov": [
        {
          "bbox": [
            234.89,
            399.49,
            549.15,
            425.9
          ],
          "page": 7,
          "span": [
            0,
            109
          ]
        }
      ]
    },
    {
      "text": "Therefore, deep queries can be implemented efficiently as long as Equation (1) can be evaluated efficiently. Over the past decades, lots of research has been conducted in the High Performance Computing community on the acceleration and parallelization of Equation (1) in the context of graphs. In this context, the matrix A is sparse and the linear operation of Equation (1) is referred to as a sparse matrix vector multiplication (SpMV), for which highly optimized implementations have been developed. 15,16 Notably, most advanced graph-analytical operations can be formulated using SpMV operations. The most trivial case is page-rank, in which one recursively executes Equation (1) in combination with a renormalization until w ! is equal to v $^{!}$. In our previous work, 2 we have also shown in detail that advanced graph-analytical operations such as node centralities and spectral analysis of the graph can be done effectively with only SpMV operations.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            279.07,
            549.01,
            379.83
          ],
          "page": 7,
          "span": [
            0,
            960
          ]
        }
      ]
    },
    {
      "text": "Since both deep queries and advanced graph analytics hugely benefit from a fast SpMV kernel, we have opted to design the graph engine in the CPS platform to work entirely with the adjacency matrix format.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            253.05,
            549.3,
            275.76
          ],
          "page": 7,
          "span": [
            0,
            204
          ]
        }
      ]
    },
    {
      "text": "3.2 | Memory architecture and performance optimization",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            43.78,
            213.48,
            380.19,
            224.87
          ],
          "page": 7,
          "span": [
            0,
            54
          ]
        }
      ]
    },
    {
      "text": "Both adjacency lists and adjacency matrices-based graph implementations have specific advantages and disadvantages. The adjacency list format is very well suited for node-centric operations since it exploits data-locality for local graph operations, such as first order traversals. However, it proves suboptimal for global scale graph operations, which are required for deep queries and the advanced graph analytics. Here, one typically has to perform graph-traversals starting from many (or even all) nodes and accumulating the weight in the resulting nodes. In an adjacency list format, this often leads to many cache misses during execution, resulting in low performance. Furthermore, parallelizing global graph-traversals in the adjacency list format suffers significantly from concurrent write conflicts between threads during execution. In the adjacency matrix format, these problems are not encountered. The graph-traversals can be directly translated into a SpMV or even a sparse-matrix sparse-vector multiplication (SpMSpV). It has also been well established how to execute the SpMV effectively in a multithreaded fashion, and how to minimize cache-misses by applying a clever sorting of the tuples list. 17",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            58.08,
            550.32,
            197.49
          ],
          "page": 7,
          "span": [
            0,
            1216
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 7,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "8of15",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            45.74,
            751.41,
            68.56,
            758.99
          ],
          "page": 8,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.92,
            758.39
          ],
          "page": 8,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "FIGURE 3 The time-to-solution for k-hop graph traversal for Neo4J and our new graph engine. The results were obtained for the graph500 and twitter benchmark graphs. The 10th and 90th percentiles are represented by the shaded regions; the median is shown by the markers",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            46.0,
            491.8,
            543.2,
            523.78
          ],
          "page": 8,
          "span": [
            0,
            268
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/2",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "To illustrate the advantages of the adjacency matrix format for our needs, we show the time-to-solution (TTS) for queries with increasing order of traversals for Neo4J \u2021\u2021 and our graph engine in Figure 3. We computed a k-hop traversal query on the graph500 \u00a7\u00a7 (64M edges) and twitter-graph \u00b6\u00b6 (1.5B edges). Two important observations can be made. Firstly, our graph engine is able to run easily third, fourth, and even higher-order graph traversals. With Neo4J, this proves very difficult, as the TTS grows upwards of 1 hour. Secondly, our graph engine shows minimal variance in the TTS between all runs of the k-order graph-traversals. This is in stark contrast to Neo4J, where the TTS strongly depends on which node(s) one starts from.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            370.06,
            551.98,
            457.64
          ],
          "page": 8,
          "span": [
            0,
            737
          ]
        }
      ]
    },
    {
      "text": "Another big advantage of using the adjacency matrix format is that we can exploit advanced compression methods 18 such as CSR or blocked COO. This reduces significantly the memory footprint of the graph and allows bigger graphs to be hosted entirely in-memory. In our case, we have opted to represent the edges by blocked matrices of a fixed size, in which each block matrix is of type COO. We chose the size of the block-matrix to be 2 16 = 65 536, allowing a pair of indices to be compactly represented by two unsigned short integers. Consequently, an edge has a memory footprint of only 4 bytes (equivalent to a single 32-bit integer), while a weighted edge a footprint of 8 bytes. *** This is a significant reduction in memory footprint compared to Neo4J graph databases, which use 33 bytes for unweighted edges $^{\u2020\u2020\u2020}$). Consequently, we can host graphs of close to 8 billion edges on a virtual machine with 32 GB of free memory, and even close to one trillion edges on a bare-metal POWER9 node with 4 TB of memory.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            239.97,
            551.49,
            366.49
          ],
          "page": 8,
          "span": [
            0,
            1021
          ]
        }
      ]
    },
    {
      "text": "3.3 | Formulation and evaluation of deep queries",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.14,
            200.5,
            333.74,
            211.89
          ],
          "page": 8,
          "span": [
            0,
            48
          ]
        }
      ]
    },
    {
      "text": "The goal of querying a KG is to answer complex questions. As such, users need to be provided with a functionality to formulate complex queries on the KG and quickly evaluate them.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.91,
            162.06,
            551.37,
            184.45
          ],
          "page": 8,
          "span": [
            0,
            179
          ]
        }
      ]
    },
    {
      "text": "In order to avoid imposing a complex query language onto users, we have devised a way to define complex graph queries in a declarative format, which we call a workflow. Workflows are represented as a DAG of operations and are conceptually related to DFs. Unlike the former, the nodes of workflow DAGs do not represent data-transformation tasks, but specific graph operations which mutate an input (or intermediate) set of nodes into another set. We call these operations worktasks. For further convenience, we have developed a graphical user interface (UI) which allows to define such workflows in a visual programming approach (see Figure 4).",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.22,
            84.05,
            550.91,
            158.49
          ],
          "page": 8,
          "span": [
            0,
            643
          ]
        }
      ]
    },
    {
      "text": "Currently, we support four fundamental types of worktasks: node-retrieval, traversal, logical operators and transform functions. In the following sections, we will discuss in detail how the worktasks are implemented in the context of our adjacency matrix design.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.99,
            45.02,
            552.19,
            80.53
          ],
          "page": 8,
          "span": [
            0,
            262
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 8,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.35,
            751.46,
            84.67,
            758.05
          ],
          "page": 9,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "9of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            528.55,
            751.41,
            550.62,
            758.05
          ],
          "page": 9,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "FIGURE 4 Visual workflow editor for deep queries in the CPS platform. The interface exhibits a left toolbar to pick specific graph operations, a main drawing area for the workflow DAG and a right panel to inspect and define parameters of each graph operation. Colors indicate different operation types such as input node-retrieval (blue), traversal (red), logical operators (green) and transform functions (yellow). Valid workflows can be executed using the ' play ' button",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            44.79,
            447.43,
            541.61,
            491.69
          ],
          "page": 9,
          "span": [
            0,
            473
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/3",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "3.3.1 | Node retrieval",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.42,
            395.52,
            176.33,
            406.91
          ],
          "page": 9,
          "span": [
            0,
            22
          ]
        }
      ]
    },
    {
      "text": "This task finds a set of nodes which satisfy certain search criteria. This can range from finding a single node by its (approximate) name or exact node identifier, to finding nodes that satisfy a particular property. The task constructs a node vector v $^{!}$, such that",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            343.81,
            548.77,
            379.57
          ],
          "page": 9,
          "span": [
            0,
            270
          ]
        }
      ]
    },
    {
      "text": "v $^{!}$$_{i}$= 1 if node i \\b S 0 if node i = 2 S , GLYPH<C26> \u00f0 3 \u00de",
      "name": "formula",
      "type": "equation",
      "prov": [
        {
          "bbox": [
            245.62,
            303.56,
            549.35,
            334.34
          ],
          "page": 9,
          "span": [
            0,
            69
          ]
        }
      ]
    },
    {
      "text": "where S represents the set of nodes that satisfy the search criteria.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.27,
            266.09,
            323.55,
            275.53
          ],
          "page": 9,
          "span": [
            0,
            69
          ]
        }
      ]
    },
    {
      "text": "3.3.2 | Graph traversal",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.09,
            226.52,
            183.25,
            237.91
          ],
          "page": 9,
          "span": [
            0,
            23
          ]
        }
      ]
    },
    {
      "text": "The simplest type of graph-traversal is the direct graph-traversal. As explained in detail in section 3.1, these can be implemented as a straightforward SpMV operation w $^{!}$= Av $^{!}$. In more advanced types of graph-traversals, we evaluate all paths of different depth. Since the number of paths connecting two nodes might increase exponentially with the pathlength, one typically reduces the contribution of each path by weighting it with the inverse factorial of the path-length. For example, consider the case in which we want to explore deeper, indirect paths as follows,",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.13,
            149.08,
            549.16,
            210.87
          ],
          "page": 9,
          "span": [
            0,
            580
          ]
        }
      ]
    },
    {
      "text": "w $^{!}$= A + A 2 2 ! + A 3 3 ! + GLYPH<C1> GLYPH<C1> GLYPH<C1> GLYPH<C18> GLYPH<C19> v $^{!}$= e$^{A}$- 1 GLYPH<C0> GLYPH<C1> v $^{!}$: \u00f0 4 \u00de",
      "name": "formula",
      "type": "equation",
      "prov": [
        {
          "bbox": [
            213.45,
            108.0,
            548.78,
            139.26
          ],
          "page": 9,
          "span": [
            0,
            142
          ]
        }
      ]
    },
    {
      "text": "In its most generic case, a graph-traversal can therefore be written down as a matrix-function applied on an edge, that is, w $^{!}$= fA \u00f0 \u00de v $^{!}$. As discussed in detail in previous work, 2 this type of operation can be evaluated extremely efficiently using a recursive Chebyshev polynomial expansion.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            45.05,
            548.8,
            80.76
          ],
          "page": 9,
          "span": [
            0,
            305
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 9,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/4",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.89,
            758.54
          ],
          "page": 10,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "3.3.3 | Logical operations",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.98,
            720.48,
            201.3,
            732.0
          ],
          "page": 10,
          "span": [
            0,
            26
          ]
        }
      ]
    },
    {
      "text": "In logical operations, two sets of nodes are merged into one resulting set, each represented through a node vector. There are three common logical operations, AND, OR, and NOT. In the AND and OR operations, we compute the geometric or the arithmetic mean respectively for each pairwise elements in the vectors. In the NOT operation, we inverse the sign for each element of the input vector.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.1,
            656.08,
            554.12,
            705.22
          ],
          "page": 10,
          "span": [
            0,
            390
          ]
        }
      ]
    },
    {
      "text": "3.3.4 | Transform functions",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.49,
            616.51,
            214.94,
            627.93
          ],
          "page": 10,
          "span": [
            0,
            27
          ]
        }
      ]
    },
    {
      "text": "Lastly, we implement operations which transform the weights associated with nodes. One such operation renormalizes and ultimately ranks the nodes according to their weight.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.36,
            578.07,
            552.45,
            600.56
          ],
          "page": 10,
          "span": [
            0,
            172
          ]
        }
      ]
    },
    {
      "text": "With these four types of operations, we can express rich queries to answer complex questions, which can have multiple inputs and outputs. Let us now discuss how a workflow is evaluated within the graph engine. Once a workflow has been submitted, each worktask is initially assigned a vector. These vectors are all initialized to zero (v $^{!}$$_{i}$= 0). Next, the graph will analyze the DAG of worktasks and identify which tasks can be run in parallel. This is achieved by performing a topological sort using depth-first traversal, which yields a list in which each item is a set of tasks that can be executed in parallel. The graph engine then proceeds with the parallel task computations.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.01,
            500.06,
            551.9,
            574.5
          ],
          "page": 10,
          "span": [
            0,
            691
          ]
        }
      ]
    },
    {
      "text": "For each task, we obtain a set of nodes with corresponding weights by identifying the nonzero elements in the associated node vector. After executing the full workflow, we therefore obtain for each task a list of nodes which can be sorted according to their weights. The higher the weight of the node, the more relevant this node is. As such, we can also retrace which nodes were important in each stage of the workflow.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.8,
            448.07,
            552.13,
            496.56
          ],
          "page": 10,
          "span": [
            0,
            420
          ]
        }
      ]
    },
    {
      "text": "4 | CLOUD DESIGN AND DEPLOYMENT",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            46.02,
            408.5,
            321.51,
            419.89
          ],
          "page": 10,
          "span": [
            0,
            31
          ]
        }
      ]
    },
    {
      "text": "The primary deployment target for the CPS is a cloud environment orchestrated via Kubernetes. We package the full platform assets with a Helm chart for quick deployment on multiple setups. For example we can easily deploy the platform on the IBM Cloud or on-premise in an IBM Cloud Private instance, both on x86-and POWER-based nodes.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.3,
            357.08,
            550.61,
            392.46
          ],
          "page": 10,
          "span": [
            0,
            334
          ]
        }
      ]
    },
    {
      "text": "In Figure 5, we show the high-level cloud design of the CPS. The platform allows to manage and instrument the corpus processing in a multitenant fashion, that is, it handles multiple knowledge ingestion pipelines and it serves multiple knowledge graphs. We call each unit a Knowledge Graph Space (KGS), which consists of a dedicated instance of the graph engine, a dedicated MongoDB database and a bucket on a cloud object store (COS). A dashboard allows each project owner to manage the access and the usage of resources. The KGS can be launched into multiple flavors to optimally balance the utilization of the cluster. These flavors range from a virtual machine with small amount of memory to a full dedicated node including hardware acceleration with GPUs. Once a KGS is created, it can be paused and rescaled without loss of data or downtime.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            253.05,
            551.04,
            353.45
          ],
          "page": 10,
          "span": [
            0,
            847
          ]
        }
      ]
    },
    {
      "text": "For the KG creation pipeline, we implemented an asynchronous compute scheme we already use in our CCS solution. 1 The system is exposed to the user via an API frontend which communicates to the compute workers through a message broker and a result backend. The workers operate on the data, which is hosted on a NoSQL database and a cloud object store for data blobs. These workers are dynamically scaled by the cloud orchestrator to best match the current load of the platform.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.44,
            188.08,
            551.4,
            249.48
          ],
          "page": 10,
          "span": [
            0,
            477
          ]
        }
      ]
    },
    {
      "text": "The processing of the KG creation typically starts with the user submitting the DF to the frontend API. The DAG of operations is then interpreted as described in the previous section and fine-grained tasks are submitted to the broker, for example, the whole corpus is split in many independent chunks. The user receives an overall status from the API and is notified when the DF processing has completed.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.28,
            136.04,
            550.96,
            184.45
          ],
          "page": 10,
          "span": [
            0,
            404
          ]
        }
      ]
    },
    {
      "text": "The KG data are distributed between three storage solutions: a NoSQL database, a cloud object storage (COS) and the KGS. Each node is represented as a document in a NoSQL database which contains all the properties attached to the node, for example, the text of a paragraph. If there is a binary object attached to the node, for example, the PDF document or an image, this is stored on the COS. The KGS contains only the minimal information needed to execute the queries, that is, the connectivity of the graph and the properties which are indexed for filtering and search.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.42,
            58.08,
            551.04,
            132.46
          ],
          "page": 10,
          "span": [
            0,
            572
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 10,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            43.99,
            751.46,
            84.67,
            758.05
          ],
          "page": 11,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "11of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            525.15,
            751.41,
            548.78,
            758.05
          ],
          "page": 11,
          "span": [
            0,
            6
          ]
        }
      ]
    },
    {
      "text": "FIGURE5 The architectural design of the CPS platform. On the left, we show the data flow processing architecture orchestrated through an asynchronous REST API. On the right, we sketch the multitenant KG serving facility which provides a dedicated environment for each project",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            44.79,
            428.34,
            541.05,
            460.56
          ],
          "page": 11,
          "span": [
            0,
            275
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/5",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "The KGS is exposed to the user via a REST API which is able to aggregate results collected from the different storage sources. To ensure decent performance when serving queries of multiple users, the graph engine can be dynamically scaled horizontally. Most workflow queries execute fast enough such that they can be responded from a synchronous request. Others, especially the graph analytics computations, are more expensive and return large amounts of data. Thus, these queries are executed through an asynchronous API and the results are paginated and streamed back to the user on completion.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            331.06,
            550.65,
            405.5
          ],
          "page": 11,
          "span": [
            0,
            596
          ]
        }
      ]
    },
    {
      "text": "5 | CASE STUDY: OIL AND GAS EXPLORATION",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.49,
            291.49,
            365.99,
            302.88
          ],
          "page": 11,
          "span": [
            0,
            39
          ]
        }
      ]
    },
    {
      "text": "Oil and gas exploration is a complex, technical field of expertise. Unfortunately, the data of many geological processes and entities is scattered across databases (public and proprietary) and corpora of documents, where it is often deeply embedded in text, tables, and figures. This is a serious impediment for efficient exploration of new oil and gas opportunities. For example, geographic information of geological structures can be found in NaturalEarthData, \u2021\u2021\u2021 while their history, evolution, and components (eg, formations with their age, rock-composition, and depth) are discussed in reports (governmental and proprietary) and scientific articles. As such, experts in oil and gas exploration often need to read many documents in order to find all the information of a certain geographic area and get a good understanding of its underlying geology.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            175.04,
            549.79,
            275.5
          ],
          "page": 11,
          "span": [
            0,
            855
          ]
        }
      ]
    },
    {
      "text": "The main tasks of the experts working in oil and gas exploration are to identify potential new exploration sites. This is typically done by describing a basin or one of its sub-regions. In practice, ' describing a basin ' boils down to identifying all geological formations with their properties in the basin and investigating if these formations constitute a petroleum system. 19 In its most minimalistic form, a petroleum system is defined by three components: source, reservoir, and seal. The source is the rock formation in which the oil or gas was created. Once created, the oil or gas typically migrates to a porous reservoir rock, which holds the oil and gas. In order for the oil and gas not to escape, the reservoir needs to be covered by an impermeable rock formation which is called the seal. Each one of these components is comprised of one or more formations, with a certain age and rock composition. To identify a petroleum system in a certain geographical area, one has to find a candidate formation for each component (ie, reservoir, seal, and source) and observe that the properties of these components satisfy some well-established constraints. For example, the reservoir",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            45.04,
            549.44,
            171.59
          ],
          "page": 11,
          "span": [
            0,
            1189
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 11,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "12",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            751.41,
            51.25,
            758.05
          ],
          "page": 12,
          "span": [
            0,
            2
          ]
        }
      ]
    },
    {
      "text": "of 15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            56.12,
            751.41,
            70.12,
            758.05
          ],
          "page": 12,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.46,
            550.74,
            758.25
          ],
          "page": 12,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "FIGURE 6 Sketch of the entire pipeline to perform deep data exploration on large corpora",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            44.77,
            585.46,
            387.12,
            593.59
          ],
          "page": 12,
          "span": [
            0,
            88
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/6",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "formation has to have a lower depth than the seal formation. Another example of such constraints is that the age of the seal and reservoir has to be older than the source.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.36,
            526.08,
            552.56,
            548.48
          ],
          "page": 12,
          "span": [
            0,
            171
          ]
        }
      ]
    },
    {
      "text": "In order for the CPS platform to help the oil and gas explorationalists in their day-to-day job effectively, it needs to meet two objectives. On the one hand, it needs to create a consistent Knowledge Graph from a document corpus. This Knowledge Graph has to contain all geological formations with their respective properties (eg, geographical locations, depth, age, and rock composition). On the other hand, CPS needs to provide fast query responses, such that one can automatically retrieve potential components of petroleum systems and apply the constraints to filter out promising candidates.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            448.07,
            552.17,
            522.45
          ],
          "page": 12,
          "span": [
            0,
            596
          ]
        }
      ]
    },
    {
      "text": "During the development and implementation of custom NLU annotators in CPS for oil and gas exploration, the client team worked hand in hand with the IBM Research team to set up a controlled accuracy benchmark in which the key capabilities of the CPS can be quantified. The goal of the benchmark was to test the entire pipeline depicted in Figure 6, that is, from PDF document ingestion to a final, queryable KG. The key components of this specific pipeline are,",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.23,
            382.82,
            552.13,
            444.6
          ],
          "page": 12,
          "span": [
            0,
            460
          ]
        }
      ]
    },
    {
      "text": "1. the conversion of PDF documents into JSON through CCS,",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            357.08,
            309.65,
            366.49
          ],
          "page": 12,
          "span": [
            0,
            57
          ]
        }
      ]
    },
    {
      "text": "2. the creation of the KG in the CPS from the JSON documents, and",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            344.04,
            336.83,
            353.64
          ],
          "page": 12,
          "span": [
            0,
            65
          ]
        }
      ]
    },
    {
      "text": "3. the querying of the KG served by CPS to identify petroleum systems elements with their properties.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.47,
            331.06,
            478.31,
            340.55
          ],
          "page": 12,
          "span": [
            0,
            101
          ]
        }
      ]
    },
    {
      "text": "On the suggestion of the experts in the client team, the entire pipeline was run on the 1051 Field Evaluation Reports from the C&C Reservoirs \u00a7\u00a7\u00a7 dataset. The advantage of using this dataset for an accuracy benchmark is that each report includes two parts. One part is verbose text describing the history, evolution, and composition of the fields. The language used is of similar complexity to standard geological publications and thus a realistic challenge for our KG creation pipeline. The second part at the end of each report is comprised of tables which summarize the text and provide us the elements of the petroleum systems with their properties. Therefore, we ingest these reports into CCS and extract both text and tables. Then, by generating a KG only from the text and keeping the tables as ground-truth to compare answers of the KG queries against, we obtain a well-controlled, end-to-end accuracy benchmark.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.17,
            214.05,
            551.78,
            314.45
          ],
          "page": 12,
          "span": [
            0,
            920
          ]
        }
      ]
    },
    {
      "text": "For step (1) of the pipeline, we ingested all 1051 PDFs into CCS and visually annotated the document structure on 300 (out of 46 019) pages. This yielded a page model which accurately converted all documents to JSON format with a 99.7% recall and 99.3% precision in the converted structure. These numbers are in line with those reported in our previous works. 1 Importantly, very accurate conversion results are key to the resulting quality, since otherwise the language annotators will process incomplete data and eventually the relevance of query results will suffer.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.26,
            149.08,
            551.37,
            210.69
          ],
          "page": 12,
          "span": [
            0,
            569
          ]
        }
      ]
    },
    {
      "text": "In step (2), we create the Knowledge Graph by executing a DF that will generate all the entities and relationships relevant to the geology domain. Our language annotator models trained for geology extract geographic areas, geological structures (eg, basins), formations, ages, rocks, petroleum systems, and their elements (PSE) (eg, seal, source, and reservoir). Overall, we extracted a total of 4597 PSEs, 8811 formations, 471 geological ages, and 64 rock types (relevant to the PSEs). The full processing performed at an average rate of 130 ms per page per worker core, on a system with three worker nodes each using four cores. Eventually, the KG included 679 296 edges connecting 116 662 nodes.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.71,
            71.07,
            551.88,
            145.51
          ],
          "page": 12,
          "span": [
            0,
            698
          ]
        }
      ]
    },
    {
      "text": "In step (3), we query the Knowledge Graph using a tailored evaluation workflow. This workflow allows us to identify PSEs and their connected properties in the Knowledge Graph, for example, their age, formation and rock",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            45.04,
            551.84,
            67.67
          ],
          "page": 12,
          "span": [
            0,
            218
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 12,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.32,
            751.46,
            84.67,
            758.05
          ],
          "page": 13,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "13",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            525.15,
            751.41,
            529.91,
            758.05
          ],
          "page": 13,
          "span": [
            0,
            2
          ]
        }
      ]
    },
    {
      "text": "of 15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            534.78,
            751.41,
            548.78,
            758.05
          ],
          "page": 13,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "FIGURE 7 The evaluation workflow to identify the petroleum system elements (PSE) in an article and infer its properties. It starts by searching for all petroleum system elements of a certain type (eg, source, reservoir or seal) and a particular report (worktasks 1 and 2). By successive graph traversals (worktasks 3-5, 7-9, 11, 12) along specific edges and logical operations (worktasks 6, 10, 13, 14), we are able to obtain a list of candidate formations (worktask 15), ages (worktask 16) and rocks (worktask 17), ranked by their accumulated weight. Execution of this query takes less than 18 ms on average",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.35,
            537.04,
            539.26,
            593.74
          ],
          "page": 13,
          "span": [
            0,
            608
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/7",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "TABLE 1 Top-k accuracies validation of KG query results. Numbers represent the fraction in which any of the k highest ranked answers matches the expected answer",
      "name": "caption",
      "type": "caption",
      "prov": [
        {
          "bbox": [
            44.49,
            441.91,
            181.12,
            498.28
          ],
          "page": 13,
          "span": [
            0,
            160
          ]
        }
      ]
    },
    {
      "$ref": "#/tables/0",
      "name": "table",
      "type": "table"
    },
    {
      "text": "composition. In Figure 7, we visualize the DAG of this workflow. The final node weights are accumulated throughout the branches on the workflow and represent the relevance score of each node.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            292.06,
            549.02,
            314.45
          ],
          "page": 13,
          "span": [
            0,
            191
          ]
        }
      ]
    },
    {
      "text": "To evaluate the correctness of the predicted PSE properties, we follow the standard practice of reporting the top-k accuracy. This is computed as the percentage in which any of the k highest ranked answers matches the expected answer, over all documents. In Table 1, we show the top-1, top-2, top-3, and top-5 accuracy for all properties of each petroleum system element. One can make two distinct observations. First, the top-1 numbers are in the range of 0.75-0.9, meaning that for 3 in 4 cases, the most relevant result predicted by the KG was correct (precision). Secondly, we observe that the top-5 numbers are very high (\u2265 0.97), showing that the system was able detect and aggregate most of the PSEs and their properties (recall). Thus, the recall of the language annotators in the KG creation pipeline was very satisfactory.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            188.08,
            550.87,
            288.53
          ],
          "page": 13,
          "span": [
            0,
            832
          ]
        }
      ]
    },
    {
      "text": "6 | CONCLUSIONS",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            44.74,
            148.51,
            178.23,
            159.9
          ],
          "page": 13,
          "span": [
            0,
            15
          ]
        }
      ]
    },
    {
      "text": "With the introduction of the CPS platform, we demonstrate substantial benefit for domain experts and data scientists in exercising deep exploration of published knowledge in a fully integrated, yet modular cloud solution. CPS seamlessly connects to the CSS, complementing it with a highly scalable, automated pipeline to build consistent domain knowledge models and an intuitive, powerful approach to explorational queries and graph-scale analytics. This is accomplished through three fundamental design considerations: (1) We do not require manual data curation or annotation; (2) We built a scalable, efficient architecture to support the ingestion, processing and query workloads, all embedded in",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            58.08,
            549.52,
            132.55
          ],
          "page": 13,
          "span": [
            0,
            699
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 13,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "14of15",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            751.41,
            70.12,
            758.05
          ],
          "page": 14,
          "span": [
            0,
            6
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            510.63,
            751.39,
            551.09,
            759.21
          ],
          "page": 14,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "text": "a single platform; and (3) We expose the capabilities through an intuitively consumable API and complementary UI tools.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.39,
            708.07,
            552.19,
            731.09
          ],
          "page": 14,
          "span": [
            0,
            119
          ]
        }
      ]
    },
    {
      "text": "In our oil and gas case study, we successfully verified our solution for a real-world application with the help of subject matter experts from a client team. Currently, CCS and CPS are actively used in more than five client engagements, most notably in the oil and gas industry as well as in the material science industry.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.29,
            669.06,
            553.28,
            705.68
          ],
          "page": 14,
          "span": [
            0,
            322
          ]
        }
      ]
    },
    {
      "text": "Future work will focus on processing public repositories such as the arXiv.org library, USPTO, and PubMed in order to make their content available to deep data exploration.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.97,
            643.04,
            553.87,
            666.64
          ],
          "page": 14,
          "span": [
            0,
            172
          ]
        }
      ]
    },
    {
      "text": "DATA AVAILABILITY STATEMENT",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            46.49,
            616.51,
            242.98,
            628.07
          ],
          "page": 14,
          "span": [
            0,
            27
          ]
        }
      ]
    },
    {
      "text": "Data subject to third party restrictions.",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            603.8,
            209.16,
            615.13
          ],
          "page": 14,
          "span": [
            0,
            41
          ]
        }
      ]
    },
    {
      "text": "ORCID",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.65,
            577.84,
            84.4,
            589.02
          ],
          "page": 14,
          "span": [
            0,
            5
          ]
        }
      ]
    },
    {
      "text": "Peter W. J. Staar https://orcid.org/0000-0002-8088-0823 Michele Dolfi https://orcid.org/0000-0001-7216-8505 Christoph Auer https://orcid.org/0000-0001-5761-0422",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.72,
            539.07,
            288.84,
            576.0
          ],
          "page": 14,
          "span": [
            0,
            160
          ]
        }
      ]
    },
    {
      "text": "ENDNOTES",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            45.98,
            512.62,
            110.58,
            524.07
          ],
          "page": 14,
          "span": [
            0,
            8
          ]
        }
      ]
    },
    {
      "text": "* For example, ElasticSearch (https://www.elastic.co) and ApacheLucene (https://lucene.apache.org).",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            498.19,
            411.12,
            507.86
          ],
          "page": 14,
          "span": [
            0,
            99
          ]
        }
      ]
    },
    {
      "text": "\u2020 Most language entities from a technical field are typically represented in a very specific, rigorous way that can be easily captured by regular expressions. We found that in practice, regular expressions often outperform DL models, since we can simply encode these representations.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.17,
            472.41,
            552.9,
            493.87
          ],
          "page": 14,
          "span": [
            0,
            283
          ]
        }
      ]
    },
    {
      "text": "\u2021 https://www.nltk.org",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.39,
            457.72,
            129.31,
            468.09
          ],
          "page": 14,
          "span": [
            0,
            22
          ]
        }
      ]
    },
    {
      "text": "\u00a7 We follow the standard JSON-schema for references.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.71,
            443.15,
            242.07,
            453.05
          ],
          "page": 14,
          "span": [
            0,
            52
          ]
        }
      ]
    },
    {
      "text": "\u00b6 A rather simple similarity metric is to perform a fuzzy comparison of the names of the newly found entities (ie, the name field found in Listing 1). A more sophisticated approach is to use word embeddings to identify if two concepts are similar.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.02,
            417.42,
            554.64,
            438.91
          ],
          "page": 14,
          "span": [
            0,
            247
          ]
        }
      ]
    },
    {
      "text": "** For example Neo4J, Titan, JanusGraph, Amazon Neptune, and Arangodb.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.49,
            402.9,
            321.26,
            412.64
          ],
          "page": 14,
          "span": [
            0,
            70
          ]
        }
      ]
    },
    {
      "text": "\u2020\u2020 This memory architecture is clearly documented for Titan (http://s3.thinkaurelius.com/docs/titan/current/data-model.html) and Neo4J (http://key-value-stories.blogspot.com/2015/02/neo4j-architecture.html).",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.0,
            376.94,
            554.38,
            398.06
          ],
          "page": 14,
          "span": [
            0,
            207
          ]
        }
      ]
    },
    {
      "text": "\u2021\u2021 We chose Neo4J as a reference since it is currently the most popular graph database solution, see https://db-engines.com/en/ranking_ trend/graph+dbms",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.06,
            350.92,
            553.26,
            372.03
          ],
          "page": 14,
          "span": [
            0,
            152
          ]
        }
      ]
    },
    {
      "text": "\u00a7\u00a7 http://graph500.org/",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.95,
            335.79,
            129.87,
            346.32
          ],
          "page": 14,
          "span": [
            0,
            23
          ]
        }
      ]
    },
    {
      "text": "\u00b6\u00b6 https://snap.stanford.edu/data/higgs-twitter.html",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            45.83,
            321.95,
            234.11,
            331.86
          ],
          "page": 14,
          "span": [
            0,
            52
          ]
        }
      ]
    },
    {
      "text": "*** We assume the weight can be represented by a float value.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.48,
            307.19,
            269.67,
            316.97
          ],
          "page": 14,
          "span": [
            0,
            61
          ]
        }
      ]
    },
    {
      "text": "\u2020\u2020\u2020 https://neo4j.com/developer/guide-sizing-and-hardware-calculator/",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.02,
            292.92,
            301.01,
            302.85
          ],
          "page": 14,
          "span": [
            0,
            69
          ]
        }
      ]
    },
    {
      "text": "\u2021\u2021\u2021 https://www.naturalearthdata.com/",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.44,
            278.17,
            187.93,
            288.11
          ],
          "page": 14,
          "span": [
            0,
            37
          ]
        }
      ]
    },
    {
      "text": "\u00a7\u00a7\u00a7 https://www.ccreservoirs.com/",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.01,
            263.8,
            169.37,
            274.13
          ],
          "page": 14,
          "span": [
            0,
            33
          ]
        }
      ]
    },
    {
      "text": "REFERENCES",
      "name": "subtitle-level-1",
      "type": "subtitle-level-1",
      "prov": [
        {
          "bbox": [
            46.05,
            231.93,
            123.27,
            244.55
          ],
          "page": 14,
          "span": [
            0,
            10
          ]
        }
      ]
    },
    {
      "text": "1. Staar Peter WJ, Michele D, Christoph A, Costas B. Corpus conversion service: a machine learning platform to ingest documents at scale. KDD '18. New York, NY: ACM; 2018:774-782.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.67,
            207.43,
            552.38,
            228.92
          ],
          "page": 14,
          "span": [
            0,
            179
          ]
        }
      ]
    },
    {
      "text": "2. Staar Peter WJ, Kl BP, Roxana I, et al. Stochastic Matrix-Function Estimators: Scalable Big-Data Kernels with High Performance. Chicago, IL: IEEE; 2016:812-821.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.74,
            184.41,
            552.62,
            205.77
          ],
          "page": 14,
          "span": [
            0,
            163
          ]
        }
      ]
    },
    {
      "text": "3. Matteo M, Christoph A, Val'ery W, et al. An information extraction and knowledge graph platform for accelerating biochemical discoveries. ArXiv.abs/1907.08400; 2019.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.74,
            161.39,
            552.68,
            182.65
          ],
          "page": 14,
          "span": [
            0,
            168
          ]
        }
      ]
    },
    {
      "text": "4. Paolo R, Marco P, Floriana B, Peter S, Costas B. Application of Geocognitive Technologies to Basin & Petroleum System Analyses, Texas: Society of Petroleum Engineers; 2019). Abu Dhabi International Petroleum Exhibition & Conference, Abu Dhabi, UAE, :10. https://doi. org/10.2118/197610-MS.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.17,
            126.92,
            552.57,
            159.62
          ],
          "page": 14,
          "span": [
            0,
            292
          ]
        }
      ]
    },
    {
      "text": "5. Guillaume L, Miguel B, Sandeep S, Kazuya K, Chris D. Neural Architectures for Named Entity Recognition, Stroudsburg PA: Association for Computational Linguistics; 2016.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.49,
            103.9,
            553.58,
            124.9
          ],
          "page": 14,
          "span": [
            0,
            171
          ]
        }
      ]
    },
    {
      "text": "6. Chiu Jason PC, Eric N. Named entity recognition with bidirectional LSTM-CNNs. TACL. 2016;4:357-370.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.74,
            92.39,
            436.99,
            101.69
          ],
          "page": 14,
          "span": [
            0,
            102
          ]
        }
      ]
    },
    {
      "text": "7. Matthew H, Ines M. spaCy 2: natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. To appear. 2017.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.74,
            69.43,
            552.49,
            90.58
          ],
          "page": 14,
          "span": [
            0,
            156
          ]
        }
      ]
    },
    {
      "text": "8. Magoon LB, Hudson TL, Peters KE. Egret-Hibernia(!), a significant petroleum system, northern Grand Banks area, offshore eastern Canada. Am Assoc Pet Geol Bull. 2005;89(9):1203-1237.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            50.38,
            46.41,
            553.17,
            67.6
          ],
          "page": 14,
          "span": [
            0,
            184
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 14,
          "span": [
            0,
            319
          ]
        }
      ]
    },
    {
      "text": "STAAR ET AL.",
      "name": "page-header",
      "type": "page-header",
      "prov": [
        {
          "bbox": [
            44.47,
            751.46,
            84.89,
            758.81
          ],
          "page": 15,
          "span": [
            0,
            12
          ]
        }
      ]
    },
    {
      "$ref": "#/figures/8",
      "name": "picture",
      "type": "figure"
    },
    {
      "text": "9. Estrada E. Subgraph centrality in complex networks. Phys Rev E. 2005;71(5):056103.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            46.63,
            722.43,
            362.75,
            731.72
          ],
          "page": 15,
          "span": [
            0,
            85
          ]
        }
      ]
    },
    {
      "text": "10. Estrada Ernesto, Higham Desmond J. (2010). Network Properties Revealed through Matrix Functions. SIAM Review, 52, (4), 696-714. http://dx.doi.org/10.1137/090761070.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            699.52,
            549.75,
            720.41
          ],
          "page": 15,
          "span": [
            0,
            168
          ]
        }
      ]
    },
    {
      "text": "11. Labs Redis. Benchmarking RedisGraph 1.0. 2019.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            688.01,
            238.67,
            697.14
          ],
          "page": 15,
          "span": [
            0,
            50
          ]
        }
      ]
    },
    {
      "text": "12. TigerGraph. Real-Time Deep Link Analytics. 2018.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.55,
            676.5,
            243.04,
            685.7
          ],
          "page": 15,
          "span": [
            0,
            52
          ]
        }
      ]
    },
    {
      "text": "13. Jeremy K, John G. Graph Algorithms in the Language of Linear Algebra. Philadelphia, PA: Society for Industrial and Applied Mathematics; 2011.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            653.54,
            548.76,
            674.38
          ],
          "page": 15,
          "span": [
            0,
            145
          ]
        }
      ]
    },
    {
      "text": "14. Kepner Jeremy, Bader David, Bulu\u00e7 Ayd \u0131 n, Gilbert John, Mattson Timothy, Meyerhenke Henning (2015). Graphs, Matrices, and the GraphBLAS: Seven Good Reasons. Procedia Computer Science, 51, 2453-2462. http://dx.doi.org/10.1016/j.procs.2015.05.353.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            630.52,
            548.83,
            651.58
          ],
          "page": 15,
          "span": [
            0,
            250
          ]
        }
      ]
    },
    {
      "text": "15. Aydin B, Gilbert John R. The combinatorial BLAS: design, implementation, and applications. Int J High Perform Comput Appl. 2011;25 (4):496-509.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            607.51,
            550.84,
            628.08
          ],
          "page": 15,
          "span": [
            0,
            147
          ]
        }
      ]
    },
    {
      "text": "16. Jeremy K, Peter A, Bader David A, et al. Mathematical foundations of the GraphBLAS. 2016 IEEE HPEC. 2016; 1-9.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            596.0,
            474.98,
            604.66
          ],
          "page": 15,
          "span": [
            0,
            114
          ]
        }
      ]
    },
    {
      "text": "17. Ariful A, Mathias J, Aydin B, Ng Esmond G. The reverse Cuthill-McKee algorithm in distributed-memory. 2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 2017: 22-31.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            573.04,
            548.8,
            592.54
          ],
          "page": 15,
          "span": [
            0,
            197
          ]
        }
      ]
    },
    {
      "text": "18. Rukhsana S, Anila U, Chughtai IR. Review of storage techniques for sparse matrices. 2005 Pakistan Section Multitopic Conference. 2005 1-7.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            550.02,
            548.72,
            569.83
          ],
          "page": 15,
          "span": [
            0,
            142
          ]
        }
      ]
    },
    {
      "text": "19. Welte DH, Horsfield B, Baker DR. Petroleum and Basin Evolution: Insights from Petroleum Geochemistry, Geology, and Basin Modeling, Berlin Heidelberg: Springer-Verlag; 1997.",
      "name": "list-item",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            44.79,
            527.0,
            550.57,
            546.75
          ],
          "page": 15,
          "span": [
            0,
            176
          ]
        }
      ]
    },
    {
      "text": "How to cite this article: Staar PWJ, Dolfi M, Auer C. Corpus processing service: A Knowledge Graph platform to perform deep data exploration on corpora. Applied AI Letters. 2020;1:e20. https://doi.org/10.1002/ail2.20",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            57.16,
            468.54,
            529.74,
            491.14
          ],
          "page": 15,
          "span": [
            0,
            216
          ]
        }
      ]
    },
    {
      "text": "26895595, 2020, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ail2.20, Wiley Online Library on [23/08/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
      "name": "text",
      "type": "paragraph",
      "prov": [
        {
          "bbox": [
            578.37,
            15.45,
            583.48,
            766.71
          ],
          "page": 15,
          "span": [
            0,
            319
          ]
        }
      ]
    }
  ],
  "page-dimensions": [
    {
      "height": 782.36,
      "page": 1,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 2,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 3,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 4,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 5,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 6,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 7,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 8,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 9,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 10,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 11,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 12,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 13,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 14,
      "width": 595.28
    },
    {
      "height": 782.36,
      "page": 15,
      "width": 595.28
    }
  ],
  "page-footers": [],
  "page-headers": [],
  "references": [],
  "tables": [
    {
      "#-cols": 6,
      "#-rows": 10,
      "confidence": 0.96,
      "created_by": "high_conf_pred",
      "type": "table",
      "cells": [],
      "data": [
        [
          {
            "bbox": [
              212.77,
              485.32,
              228.25,
              493.39
            ],
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                0
              ]
            ],
            "text": "PSE",
            "type": ""
          },
          {
            "bbox": [
              280.46,
              485.32,
              315.04,
              493.39
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                1
              ]
            ],
            "text": "Property",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              485.32,
              374.29,
              493.39
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                2
              ]
            ],
            "text": "Top-1",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              485.32,
              430.13,
              493.39
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                3
              ]
            ],
            "text": "Top-2",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              485.32,
              485.97,
              493.39
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                4
              ]
            ],
            "text": "Top-3",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              485.32,
              541.76,
              493.39
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 0,
            "row-header": false,
            "row-span": [
              0,
              1
            ],
            "spans": [
              [
                0,
                5
              ]
            ],
            "text": "Top-5",
            "type": ""
          }
        ],
        [
          {
            "bbox": [
              212.77,
              469.69,
              246.57,
              477.69
            ],
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                0
              ]
            ],
            "text": "Reservoir",
            "type": ""
          },
          {
            "bbox": [
              280.46,
              469.69,
              294.44,
              477.69
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                1
              ]
            ],
            "text": "Age",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              469.69,
              366.85,
              477.69
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                2
              ]
            ],
            "text": "0.82",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              469.69,
              422.69,
              477.69
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                3
              ]
            ],
            "text": "0.96",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              469.69,
              478.54,
              477.69
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                4
              ]
            ],
            "text": "0.98",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              469.69,
              534.32,
              477.69
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 1,
            "row-header": false,
            "row-span": [
              1,
              2
            ],
            "spans": [
              [
                1,
                5
              ]
            ],
            "text": "1.00",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              454.66,
              318.41,
              462.66
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                1
              ]
            ],
            "text": "Formation",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              454.66,
              366.85,
              462.66
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                2
              ]
            ],
            "text": "0.93",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              454.66,
              422.69,
              462.66
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                3
              ]
            ],
            "text": "0.98",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              454.66,
              478.54,
              462.66
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                4
              ]
            ],
            "text": "1.00",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              454.66,
              534.32,
              462.66
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 2,
            "row-header": false,
            "row-span": [
              2,
              3
            ],
            "spans": [
              [
                2,
                5
              ]
            ],
            "text": "1.00",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              439.64,
              298.68,
              447.64
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                1
              ]
            ],
            "text": "Rock",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              439.64,
              366.85,
              447.64
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                2
              ]
            ],
            "text": "0.62",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              439.64,
              422.69,
              447.64
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                3
              ]
            ],
            "text": "0.80",
            "type": ""
          },
          {
            "bbox": [
              464.03,
              439.64,
              478.54,
              447.64
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                4
              ]
            ],
            "text": "0.87",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              439.64,
              534.32,
              447.64
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 3,
            "row-header": false,
            "row-span": [
              3,
              4
            ],
            "spans": [
              [
                3,
                5
              ]
            ],
            "text": "0.94",
            "type": ""
          }
        ],
        [
          {
            "bbox": [
              212.77,
              424.67,
              227.37,
              432.67
            ],
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                0
              ]
            ],
            "text": "Seal",
            "type": ""
          },
          {
            "bbox": [
              280.46,
              424.67,
              294.44,
              432.67
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                1
              ]
            ],
            "text": "Age",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              424.67,
              366.85,
              432.67
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                2
              ]
            ],
            "text": "0.73",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              424.67,
              422.69,
              432.67
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                3
              ]
            ],
            "text": "0.91",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              424.67,
              478.54,
              432.67
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                4
              ]
            ],
            "text": "0.94",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              424.67,
              534.32,
              432.67
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 4,
            "row-header": false,
            "row-span": [
              4,
              5
            ],
            "spans": [
              [
                4,
                5
              ]
            ],
            "text": "0.97",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              409.65,
              318.41,
              417.65
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                1
              ]
            ],
            "text": "Formation",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              409.65,
              366.85,
              417.65
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                2
              ]
            ],
            "text": "0.82",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              409.65,
              422.69,
              417.65
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                3
              ]
            ],
            "text": "0.94",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              409.65,
              478.54,
              417.65
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                4
              ]
            ],
            "text": "0.97",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              409.65,
              534.32,
              417.65
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 5,
            "row-header": false,
            "row-span": [
              5,
              6
            ],
            "spans": [
              [
                5,
                5
              ]
            ],
            "text": "0.98",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              394.68,
              298.68,
              402.68
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                1
              ]
            ],
            "text": "Rock",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              394.68,
              366.85,
              402.68
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                2
              ]
            ],
            "text": "0.82",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              394.68,
              422.69,
              402.68
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                3
              ]
            ],
            "text": "0.92",
            "type": ""
          },
          {
            "bbox": [
              464.03,
              394.68,
              478.54,
              402.68
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                4
              ]
            ],
            "text": "0.95",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              394.68,
              534.32,
              402.68
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 6,
            "row-header": false,
            "row-span": [
              6,
              7
            ],
            "spans": [
              [
                6,
                5
              ]
            ],
            "text": "0.97",
            "type": ""
          }
        ],
        [
          {
            "bbox": [
              212.77,
              379.66,
              236.93,
              387.66
            ],
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                0
              ]
            ],
            "text": "Source",
            "type": ""
          },
          {
            "bbox": [
              280.46,
              379.66,
              294.44,
              387.66
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                1
              ]
            ],
            "text": "Age",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              379.66,
              366.85,
              387.66
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                2
              ]
            ],
            "text": "0.75",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              379.66,
              422.69,
              387.66
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                3
              ]
            ],
            "text": "0.92",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              379.66,
              478.54,
              387.66
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                4
              ]
            ],
            "text": "0.96",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              379.66,
              534.32,
              387.66
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 7,
            "row-header": false,
            "row-span": [
              7,
              8
            ],
            "spans": [
              [
                7,
                5
              ]
            ],
            "text": "0.97",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              364.69,
              318.41,
              372.69
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                1
              ]
            ],
            "text": "Formation",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              364.69,
              366.85,
              372.69
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                2
              ]
            ],
            "text": "0.89",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              364.69,
              422.69,
              372.69
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                3
              ]
            ],
            "text": "0.96",
            "type": ""
          },
          {
            "bbox": [
              464.04,
              364.69,
              478.54,
              372.69
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                4
              ]
            ],
            "text": "0.97",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              364.69,
              534.32,
              372.69
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 8,
            "row-header": false,
            "row-span": [
              8,
              9
            ],
            "spans": [
              [
                8,
                5
              ]
            ],
            "text": "0.98",
            "type": ""
          }
        ],
        [
          {
            "bbox": null,
            "col": 0,
            "col-header": false,
            "col-span": [
              0,
              1
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                0
              ]
            ],
            "text": "",
            "type": "body"
          },
          {
            "bbox": [
              280.46,
              349.67,
              298.68,
              357.67
            ],
            "col": 1,
            "col-header": false,
            "col-span": [
              1,
              2
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                1
              ]
            ],
            "text": "Rock",
            "type": ""
          },
          {
            "bbox": [
              352.35,
              349.67,
              366.85,
              357.67
            ],
            "col": 2,
            "col-header": false,
            "col-span": [
              2,
              3
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                2
              ]
            ],
            "text": "0.83",
            "type": ""
          },
          {
            "bbox": [
              408.19,
              349.67,
              422.69,
              357.67
            ],
            "col": 3,
            "col-header": false,
            "col-span": [
              3,
              4
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                3
              ]
            ],
            "text": "0.92",
            "type": ""
          },
          {
            "bbox": [
              464.03,
              349.67,
              478.54,
              357.67
            ],
            "col": 4,
            "col-header": false,
            "col-span": [
              4,
              5
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                4
              ]
            ],
            "text": "0.95",
            "type": ""
          },
          {
            "bbox": [
              519.82,
              349.67,
              534.32,
              357.67
            ],
            "col": 5,
            "col-header": false,
            "col-span": [
              5,
              6
            ],
            "row": 9,
            "row-header": false,
            "row-span": [
              9,
              10
            ],
            "spans": [
              [
                9,
                5
              ]
            ],
            "text": "0.96",
            "type": ""
          }
        ]
      ],
      "text": "TABLE 1 Top-k accuracies validation of KG query results. Numbers represent the fraction in which any of the k highest ranked answers matches the expected answer",
      "prov": [
        {
          "bbox": [
            210.0,
            346.58,
            549.02,
            499.13
          ],
          "page": 13,
          "span": [
            0,
            0
          ]
        }
      ]
    }
  ],
  "conversion_settings": {
    "model_pipeline": {
      "clusters": [
        {
          "type": "LayoutSegmentationModel",
          "name": "LayoutSegmentationModel",
          "version": "NA"
        }
      ],
      "page": [],
      "normalization": [],
      "tables": [
        {
          "type": "TableStructureModel",
          "name": "TableStructureModel",
          "version": "NA"
        }
      ]
    }
  },
  "version": 2
}